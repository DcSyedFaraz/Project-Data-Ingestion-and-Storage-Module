#!/usr/bin/env bash
set -euo pipefail

###############################################################################
# Local pipeline launcher (pandasâ€‘only)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  1. Starts core containers via DockerÂ Compose (API, auth, modelâ€‘serving, â€¦)
#  2. Downloads the public climate dataset into ./datasets
#  3. Builds the lightweight ML image (PythonÂ +Â pandasÂ +Â scikitâ€‘learn)
#  4. Executes batch aggregation â†’ feature prep â†’ model training in one shot
#  5. Drops the trained model into ./model_serving/models
#  6. Smokeâ€‘tests the /predict endpoint
###############################################################################

# 1Â Â Start services defined in dockerâ€‘compose.yml
printf '\nâ–¶ï¸Ž  Starting DockerÂ Compose servicesâ€¦\n'
# docker-compose up --build -d

# # 2Â Â Fetch sample data (writes to ./datasets)
# printf '\nâ–¶ï¸Ž  Downloading climate datasetâ€¦\n'
# ./scripts/download_sample_data.sh

# # 3Â Â Build (or rebuild) the ML pipeline image
# printf '\nâ–¶ï¸Ž  Building ml_pipeline imageâ€¦\n'
# docker build \
#   -t yourrepo/ml_pipeline:latest \
#   -f "ml_pipeline/Dockerfile" \
#   .
# # 4Â Â Run pandas batchâ€‘agg, preprocessing, and training inside the container
# printf '\nâ–¶ï¸Ž  Preprocessing and training with pandasâ€¦\n'
# mkdir -p model_serving/models # hostâ€‘side output directory

# docker run --rm \
#   -v "$(pwd)/datasets:/data" \
#   -v "$(pwd)/model_serving/models:/app/models" \
#   yourrepo/ml_pipeline:latest \
#   bash -e -c $'
#     # â‘  batch aggregation
#     python batch_processing/pandas_batch_job.py \
#       --input  /data/global_temp.json \
#       --output /data/processed.parquet

#     # â‘¡ feature preparation
#     python ml_pipeline/pandas_preprocessing.py  --input  /data/processed.parquet --output /tmp/ml_ready.parquet

#     # â‘¢ model training
#     python ml_pipeline/model_training.py --data /tmp/ml_ready.parquet --model /app/models/temperature_model.joblib
#   '

# printf '\nâœ…  Model saved to model_serving/models/temperature_model.joblib\n'

# 5Â Â Smokeâ€‘test the prediction endpoint
printf '\nâ–¶ï¸Ž  Testing /predict endpointâ€¦\n'
PRED=$(curl -s -X POST http://localhost:8000/predict \
  -H 'Content-Type: application/json' \
  -d '{"year": 2025, "month": 7}')

echo "Prediction response: $PRED"
printf '\nðŸŽ‰  Local environment setup and validation complete.\n'
